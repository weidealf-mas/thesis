#!/usr/bin/python

"""
Consume log entries (generated by a Swiss Identity Provider) from Apache Kafka and predict
with different algorithms in real-time
"""

import sys
sys.path.append('../ml_utils')

import pandas as pd

from pyspark.sql.functions import *
from pyspark import SparkContext, SparkConf
from pyspark.sql import SQLContext, SparkSession


# Import our Config file and our Pre-Processing and Feature Vectorisation pipeline functions
import config as cfg
import spark_utils as su
import sentiment_classifier as sent_clf


__author__ = "Fredi Weideli"
__credits__ = ["Fredi Weideli"]
__version__ = "1.0.0"
__maintainer__ = "Fredi Weideli"
__email__ = "fredi.weideli@bluewin.ch"
__status__ = "Master-thesis-POC"


# create a spark session using the spark context instantiated from spark-submit
# spark = SparkSession.builder.appName("Real-Time SwissID Anomaly Detection").getOrCreate()
conf = SparkConf().setMaster("spark://spark-master:7077").setAppName("Real-Time SwissID Anomaly Detection")

# sc = spark.sparkContext
sc = SparkContext(conf=conf)
sqlContext = SQLContext(sc)

# spark = SparkSession.builder.enableHiveSupport().getOrCreate()
spark = SparkSession.builder.getOrCreate()

print(sc.getConf().getAll() == spark.sparkContext.getConf().getAll())
print(sc.getConf().getAll())

sc.setLogLevel("WARN")

# spark.conf.set("spark.sql.debug.maxToStringFields", 1000)


out_schema = "prediction string, classifier string, log_stmt_uuid string, log_stmt_json string"

@pandas_udf(out_schema, PandasUDFType.GROUPED_MAP)
def ovsvm_predict_udf(pdf):
    # pdf is a pandas.DataFrame

    import joblib

    path_clf = '/shared/models/ocsvm/optimized_ocsvm_encoded_clf_all.pkl'
    path_enc = '/shared/models/ocsvm/optimized_ocsvm_encoder_all.pkl'

    clf = joblib.load(path_clf)
    enc_dict = joblib.load(path_enc)

    ids = pdf['log_stmt_uuid'].tolist()
    stmts = pdf['log_stmt_json'].tolist()

    df = pdf.drop(labels=['label_nr', 'log_stmt_uuid', 'log_stmt_json'], axis=1)
    df['date_weekday'] = df['date_weekday'].astype(int)
    df['response_status_code'] = df['response_status_code'].astype(int)

    for col in df.columns:
        if df[col].dtype == "object":
            enc = enc_dict.get(col, None)
            col_val = df[col].values.reshape(-1, 1)
            df[col] = enc.transform(col_val)

    predictions = clf.predict(df)

    pred_string = []
    for i in range(0, len(predictions)):
        if predictions[i] == 1:
            pred_string.append('normal')
        else:
            pred_string.append('anomaly')

    pred_df = pd.DataFrame(pred_string, columns=['prediction'])
    pred_df['classifier'] = pd.Series(['ocsvm']*len(pred_df))
    pred_df['log_stmt_uuid'] = ids
    pred_df['log_stmt_json'] = stmts

    return pred_df


print("************************")
print("* Start Anomaly-Detector")
print("************************")


# subscribe to the Kafka 'idp_events' topic and consume log entries into an unbounded spark dataframe
log_entries_df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", cfg.bootstrap_servers) \
    .option("subscribe", cfg.log_entries_kafka_topic) \
    .load()

# cast the value field (containing log entries) as STRING
# then apply the defined schema to parse the value string as JSON
log_entries_df = log_entries_df.selectExpr("CAST(key AS STRING) as " + cfg.key_col_name, "CAST(value AS STRING) as " + cfg.value_col_name)\
    .select(from_json(col(cfg.value_col_name), su.feature_schema_all_string).alias('log_stmt_columns'), col(cfg.key_col_name),  col(cfg.value_col_name))\
    .select('log_stmt_columns.*', cfg.key_col_name, cfg.value_col_name)

# prepare (clean) the log-statements
reduced_feature_set = True
log_entries_df = su.clean_log_entries(log_entries_df, False, True, True, reduced_feature_set, False)
print(log_entries_df.printSchema())


# predict with one-class svm
ocsvm_prediction_df = log_entries_df.groupby(cfg.key_col_name).apply(ovsvm_predict_udf)

# predict with sentiment
sentiment_prediction_df = sent_clf.detect(log_entries_df, '/shared/models/sentiment/overall/optimized', True, False, reduced_feature_set)

# union the prediction result
predictions_df = sentiment_prediction_df.union(ocsvm_prediction_df)

# just select anomalies and count how many algorithm agrees
predictions_df = predictions_df.filter("prediction == 'anomaly'").groupby(cfg.key_col_name, cfg.value_col_name).agg(count("*").alias('vote'))

# in this case (example) we just have two voters (should be uneven!), but good enough for now
# it only an anomaly when both say 'yes', then convert the dataframe to a JSON string and hand it over to Kafka
predictions_df = predictions_df.filter("vote > 1").select(concat(lit('{"vote":"'), col("vote"), lit('","uuid":"'),
                                                        col(cfg.key_col_name), lit('","log_stmt":"'), col(cfg.value_col_name), lit('"}')).alias("value"))

# write the result to Kafka
predictions_query = predictions_df \
    .writeStream \
    .queryName("predictions_query") \
    .format("kafka") \
    .option("kafka.bootstrap.servers", cfg.bootstrap_servers) \
    .option("topic", cfg.anomaly_kafka_topic) \
    .option("startingOffsets", "latest") \
    .trigger(processingTime='5 seconds') \
    .option("checkpointLocation", cfg.checkpoint_path) \
    .option("failOnDataLoss", "false") \
    .outputMode("update") \
    .start()

predictions_query.awaitTermination()

print("Anomaly-Detector terminated!")
