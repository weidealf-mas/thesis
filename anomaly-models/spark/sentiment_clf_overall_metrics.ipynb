{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/administrator/anaconda3/envs/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/administrator/anaconda3/envs/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/administrator/anaconda3/envs/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/administrator/anaconda3/envs/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/administrator/anaconda3/envs/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/administrator/anaconda3/envs/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../ml_utils')\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from handyspark import *\n",
    "\n",
    "import data_utils as du\n",
    "import spark_utils as su\n",
    "import sentiment_classifier\n",
    "import timeit\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create Spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_endpoint = \"spark://spark.home.net:7077\"\n",
    "#base_dir = '/Users/administrator/'\n",
    "\n",
    "spark_endpoint = \"spark://lasvegas:7077\"\n",
    "base_dir = '/home/administrator/'\n",
    "\n",
    "# these two jars must be added!\n",
    "spark_jars = base_dir + 'Development/spark-2.4.4-bin-hadoop2.7/jars/spark-nlp_2.11-2.3.3.jar,' + base_dir + '/Development/spark-2.4.4-bin-hadoop2.7/jars/config-1.4.0.jar'\n",
    "\n",
    "conf = SparkConf().setMaster(spark_endpoint) \\\n",
    ".set(\"spark.jars\", spark_jars) \\\n",
    ".setAppName(\"Sentiment Analysis\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "#sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_entries_df = sqlContext.read.format('com.databricks.spark.csv') \\\n",
    "    .schema(su.feature_schema) \\\n",
    "    .options(header = 'false', inferschema = 'false', delimiter = '\\t') \\\n",
    "    .load('./../../shared/data/swissid_authorize_logs_april_to_sept_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_feature_set = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = su.clean_log_entries(log_entries_df, False, False, False, reduced_feature_set, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df = reduced_df.filter(\"label_nr = 0.0\")\n",
    "anomaly_df = reduced_df.filter(\"label_nr = 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a sampled dataframe having the same anomaly rate as the original dateset and collect the metrics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "metrics = []\n",
    "iter_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Time to fit: 42.19 seconds\n",
      "1 Time to fit: 35.17 seconds\n",
      "2 Time to fit: 35.38 seconds\n",
      "3 Time to fit: 34.75 seconds\n",
      "4 Time to fit: 34.95 seconds\n",
      "5 Time to fit: 34.94 seconds\n",
      "6 Time to fit: 34.90 seconds\n",
      "7 Time to fit: 34.74 seconds\n",
      "8 Time to fit: 34.73 seconds\n",
      "9 Time to fit: 34.86 seconds\n",
      "10 Time to fit: 34.91 seconds\n",
      "11 Time to fit: 34.94 seconds\n",
      "12 Time to fit: 34.65 seconds\n",
      "13 Time to fit: 34.61 seconds\n",
      "14 Time to fit: 34.46 seconds\n",
      "15 Time to fit: 34.09 seconds\n",
      "16 Time to fit: 33.81 seconds\n",
      "17 Time to fit: 34.29 seconds\n",
      "18 Time to fit: 34.00 seconds\n",
      "19 Time to fit: 34.43 seconds\n",
      "20 Time to fit: 34.37 seconds\n",
      "21 Time to fit: 34.64 seconds\n",
      "22 Time to fit: 34.60 seconds\n",
      "23 Time to fit: 34.78 seconds\n",
      "24 Time to fit: 34.32 seconds\n",
      "25 Time to fit: 34.58 seconds\n",
      "26 Time to fit: 34.57 seconds\n",
      "27 Time to fit: 34.64 seconds\n",
      "28 Time to fit: 34.47 seconds\n",
      "29 Time to fit: 34.69 seconds\n",
      "30 Time to fit: 34.78 seconds\n",
      "31 Time to fit: 34.63 seconds\n",
      "32 Time to fit: 34.76 seconds\n",
      "33 Time to fit: 34.30 seconds\n",
      "34 Time to fit: 34.53 seconds\n",
      "35 Time to fit: 34.58 seconds\n",
      "36 Time to fit: 34.67 seconds\n",
      "37 Time to fit: 34.57 seconds\n",
      "38 Time to fit: 34.78 seconds\n",
      "39 Time to fit: 34.90 seconds\n",
      "40 Time to fit: 34.60 seconds\n",
      "41 Time to fit: 34.58 seconds\n",
      "42 Time to fit: 34.79 seconds\n",
      "43 Time to fit: 34.28 seconds\n",
      "44 Time to fit: 34.09 seconds\n",
      "45 Time to fit: 34.23 seconds\n",
      "46 Time to fit: 34.63 seconds\n",
      "47 Time to fit: 34.04 seconds\n",
      "48 Time to fit: 34.51 seconds\n",
      "49 Time to fit: 34.29 seconds\n",
      "50 Time to fit: 33.87 seconds\n",
      "51 Time to fit: 34.02 seconds\n",
      "52 Time to fit: 34.07 seconds\n",
      "53 Time to fit: 34.35 seconds\n",
      "54 Time to fit: 33.89 seconds\n",
      "55 Time to fit: 33.96 seconds\n",
      "56 Time to fit: 33.80 seconds\n",
      "57 Time to fit: 33.85 seconds\n",
      "58 Time to fit: 34.21 seconds\n",
      "59 Time to fit: 33.93 seconds\n",
      "60 Time to fit: 34.06 seconds\n",
      "61 Time to fit: 33.99 seconds\n",
      "62 Time to fit: 33.68 seconds\n",
      "63 Time to fit: 33.91 seconds\n",
      "64 Time to fit: 33.92 seconds\n",
      "65 Time to fit: 33.72 seconds\n",
      "66 Time to fit: 33.76 seconds\n",
      "67 Time to fit: 33.97 seconds\n",
      "68 Time to fit: 34.37 seconds\n",
      "69 Time to fit: 34.25 seconds\n",
      "70 Time to fit: 34.30 seconds\n",
      "71 Time to fit: 34.43 seconds\n",
      "72 Time to fit: 34.49 seconds\n",
      "73 Time to fit: 34.64 seconds\n",
      "74 Time to fit: 34.65 seconds\n",
      "75 Time to fit: 34.60 seconds\n",
      "76 Time to fit: 33.78 seconds\n",
      "77 Time to fit: 34.13 seconds\n",
      "78 Time to fit: 33.94 seconds\n",
      "79 Time to fit: 34.00 seconds\n",
      "80 Time to fit: 33.83 seconds\n",
      "81 Time to fit: 33.97 seconds\n",
      "82 Time to fit: 34.09 seconds\n",
      "83 Time to fit: 33.97 seconds\n",
      "84 Time to fit: 34.25 seconds\n",
      "85 Time to fit: 34.01 seconds\n",
      "86 Time to fit: 34.12 seconds\n",
      "87 Time to fit: 34.18 seconds\n",
      "88 Time to fit: 34.45 seconds\n",
      "89 Time to fit: 34.61 seconds\n",
      "90 Time to fit: 34.48 seconds\n",
      "91 Time to fit: 34.39 seconds\n",
      "92 Time to fit: 34.41 seconds\n",
      "93 Time to fit: 34.55 seconds\n",
      "94 Time to fit: 34.31 seconds\n",
      "95 Time to fit: 34.55 seconds\n",
      "96 Time to fit: 34.27 seconds\n",
      "97 Time to fit: 34.01 seconds\n",
      "98 Time to fit: 34.50 seconds\n",
      "99 Time to fit: 34.45 seconds\n",
      "Time: 8300.96 seconds\n",
      "\n",
      "Time: 138.35 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_size_percentage = 0.041\n",
    "\n",
    "start_overall = timeit.default_timer()\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    x_df = normal_df.sample(False, sample_size_percentage).union(anomaly_df.sample(False, sample_size_percentage))\n",
    "    \n",
    "    preprocessed_df = sentiment_classifier.preprocess(x_df, False, reduced_feature_set)\n",
    "    encoded_features_df = sentiment_classifier.vectorize(preprocessed_df, False)\n",
    "    \n",
    "    train_df, test_df = encoded_features_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    clf = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label_nr', maxDepth=6, maxBins=25)\n",
    "    \n",
    "    start_fitting = timeit.default_timer()\n",
    "    model = clf.fit(train_df)\n",
    "    stop_fitting = timeit.default_timer()\n",
    "    \n",
    "    fit_time = stop_fitting - start_fitting\n",
    "    print(\"{0} Time to fit: {1:.2f} seconds\".format(i, fit_time))\n",
    "    \n",
    "    \n",
    "    predictions_df = model.transform(test_df)\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator(labelCol='label_nr')\n",
    "    auc = evaluator.evaluate(predictions_df)\n",
    "\n",
    "    predictions_and_label = predictions_df.select(\"prediction\", \"label_nr\").rdd\n",
    "    class_metrics = MulticlassMetrics(predictions_and_label)\n",
    "\n",
    "    tot = predictions_df.count()\n",
    "    cm = class_metrics.confusionMatrix().toArray()\n",
    "    t_pos = cm[0][0]\n",
    "    f_pos = cm[0][1]\n",
    "    f_neg = cm[1][0]\n",
    "    t_neg = cm[1][1]\n",
    "\n",
    "    acc = (t_pos + t_neg) / tot\n",
    "\n",
    "    f1 = class_metrics.fMeasure(1.0)\n",
    "    p = class_metrics.precision(1.0)\n",
    "    r = class_metrics.recall(1.0)\n",
    "\n",
    "    iter_values = [tot, r, p, acc, f1, t_pos, f_pos, f_neg, t_neg, auc, fit_time ]\n",
    "    \n",
    "    metrics.append(iter_values)\n",
    "    \n",
    "    iter_values = []\n",
    "\n",
    "stop_overall = timeit.default_timer()\n",
    "\n",
    "overall_runtime = stop_overall - start_overall\n",
    "print(\"Time: {0:.2f} seconds\\n\".format(overall_runtime))\n",
    "print(\"Time: {0:.2f} minutes\\n\".format(overall_runtime/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>tpr_recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>t_pos</th>\n",
       "      <th>f_pos</th>\n",
       "      <th>f_neg</th>\n",
       "      <th>t_neg</th>\n",
       "      <th>auc</th>\n",
       "      <th>time_to_fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39295.960000</td>\n",
       "      <td>0.996348</td>\n",
       "      <td>0.997585</td>\n",
       "      <td>0.999972</td>\n",
       "      <td>0.996957</td>\n",
       "      <td>39112.040000</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>182.810000</td>\n",
       "      <td>0.997661</td>\n",
       "      <td>34.449345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>87.616234</td>\n",
       "      <td>0.004632</td>\n",
       "      <td>0.003442</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.002710</td>\n",
       "      <td>86.793488</td>\n",
       "      <td>0.624742</td>\n",
       "      <td>0.853454</td>\n",
       "      <td>7.393274</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.859738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>39075.000000</td>\n",
       "      <td>0.978261</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>0.999898</td>\n",
       "      <td>0.988827</td>\n",
       "      <td>38888.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>0.983734</td>\n",
       "      <td>33.677653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39241.500000</td>\n",
       "      <td>0.994536</td>\n",
       "      <td>0.994645</td>\n",
       "      <td>0.999949</td>\n",
       "      <td>0.994645</td>\n",
       "      <td>39051.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>0.997147</td>\n",
       "      <td>34.055520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39303.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.997275</td>\n",
       "      <td>39119.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>0.997340</td>\n",
       "      <td>34.417872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>39352.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39165.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>34.631278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>39609.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39423.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>42.186243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              total  tpr_recall   precision    accuracy    f1_score  \\\n",
       "count    100.000000  100.000000  100.000000  100.000000  100.000000   \n",
       "mean   39295.960000    0.996348    0.997585    0.999972    0.996957   \n",
       "std       87.616234    0.004632    0.003442    0.000025    0.002710   \n",
       "min    39075.000000    0.978261    0.988764    0.999898    0.988827   \n",
       "25%    39241.500000    0.994536    0.994645    0.999949    0.994645   \n",
       "50%    39303.000000    1.000000    1.000000    0.999975    0.997275   \n",
       "75%    39352.250000    1.000000    1.000000    1.000000    1.000000   \n",
       "max    39609.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "              t_pos       f_pos       f_neg       t_neg         auc  \\\n",
       "count    100.000000  100.000000  100.000000  100.000000  100.000000   \n",
       "mean   39112.040000    0.440000    0.670000  182.810000    0.997661   \n",
       "std       86.793488    0.624742    0.853454    7.393274    0.002721   \n",
       "min    38888.000000    0.000000    0.000000  166.000000    0.983734   \n",
       "25%    39051.750000    0.000000    0.000000  178.000000    0.997147   \n",
       "50%    39119.500000    0.000000    0.000000  182.000000    0.997340   \n",
       "75%    39165.000000    1.000000    1.000000  187.000000    1.000000   \n",
       "max    39423.000000    2.000000    4.000000  201.000000    1.000000   \n",
       "\n",
       "       time_to_fit  \n",
       "count   100.000000  \n",
       "mean     34.449345  \n",
       "std       0.859738  \n",
       "min      33.677653  \n",
       "25%      34.055520  \n",
       "50%      34.417872  \n",
       "75%      34.631278  \n",
       "max      42.186243  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df = du.create_sentiment_metric_df(metrics, './sentiment_clf_overall_metrics.csv')\n",
    "metrics_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark Context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
