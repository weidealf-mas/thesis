{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/administrator/anaconda3/envs/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/administrator/anaconda3/envs/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/administrator/anaconda3/envs/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/administrator/anaconda3/envs/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/administrator/anaconda3/envs/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/administrator/anaconda3/envs/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../ml_utils')\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import timeit\n",
    "import data_utils\n",
    "import spark_utils as su\n",
    "import sentiment_classifier\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import LongType, DoubleType, IntegerType, StringType, BooleanType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "if platform.system() == 'Darwin':\n",
    "    spark_master = \"spark://spark.home.net:7077\"\n",
    "    base_dir = '/Users/administrator/'\n",
    "else:\n",
    "    spark_master = \"spark://lasvegas:7077\"\n",
    "    base_dir = '/home/administrator/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(spark_master).setAppName(\"Logistic Regression\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "#sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the log statements based on a given schema\n",
    "log_entries_df = sqlContext.read.format('com.databricks.spark.csv') \\\n",
    "    .schema(su.feature_schema) \\\n",
    "    .options(header = 'false', inferschema = 'false', delimiter = '\\t') \\\n",
    "    .load('./../../shared/data/swissid_authorize_logs_april_to_sept_2019.csv')\n",
    "\n",
    "#log_entries_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter 'bad' statements and select a subset of all features\n",
    "filtered_df = su.clean_log_entries(log_entries_df, True, False, False)\n",
    "\n",
    "#filtered_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: string (nullable = true)\n",
      " |-- label_nr: double (nullable = true)\n",
      " |-- date_weekday: integer (nullable = true)\n",
      " |-- date_hour: integer (nullable = true)\n",
      " |-- src_software_name: string (nullable = true)\n",
      " |-- src_operating_system_name: string (nullable = true)\n",
      " |-- src_software_type: string (nullable = true)\n",
      " |-- src_hardware_type: string (nullable = true)\n",
      " |-- http_method: string (nullable = true)\n",
      " |-- response_status: string (nullable = true)\n",
      " |-- oidc_response_type: string (nullable = true)\n",
      " |-- oidc_acr_values: string (nullable = true)\n",
      " |-- oidc_client_id: string (nullable = true)\n",
      " |-- client_type: string (nullable = true)\n",
      " |-- ido_id: string (nullable = true)\n",
      " |-- oidc_scopes: string (nullable = true)\n",
      " |-- oidc_ui_locales: string (nullable = true)\n",
      " |-- loc_country: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = filtered_df.schema.names\n",
    "categorical_columns.remove('label')\n",
    "categorical_columns.remove('label_nr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_stages = []\n",
    "\n",
    "for col in categorical_columns:\n",
    "    string_indexer = StringIndexer(inputCol = col, outputCol = col + '_idx')\n",
    "    encoder = OneHotEncoderEstimator(inputCols = [string_indexer.getOutputCol()], outputCols = [col + \"_ohe\"])\n",
    "    pipeline_stages += [string_indexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_string_indexer = StringIndexer(inputCol = 'label', outputCol = 'label_idx')\n",
    "#pipeline_stages += [label_string_indexer]\n",
    "\n",
    "vector_assembler_inputs = [c + \"_ohe\" for c in categorical_columns]\n",
    "vector_assembler = VectorAssembler(inputCols = vector_assembler_inputs, outputCol = \"features\")\n",
    "pipeline_stages += [vector_assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 4.875693701000046 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages = pipeline_stages)\n",
    "\n",
    "pipeline_model = pipeline.fit(filtered_df)\n",
    "\n",
    "#label_column = 'label_idx'\n",
    "start = timeit.default_timer()\n",
    "filtered_features_df = pipeline_model.transform(filtered_df).select(['features', 'label_nr', 'label'])\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print(\"Time: {} seconds\\n\".format(stop - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_sample = filtered_features_df.filter(\"label = 'anomaly'\")\n",
    "anomaly_sample_count = anomaly_sample.count()\n",
    "#anomaly_sample.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df = filtered_features_df.filter(\"label = 'normal'\")\n",
    "normal_count = normal_df.count()\n",
    "normal_sample = normal_df.sample(False, anomaly_sample_count/normal_count)\n",
    "#normal_sample.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45347"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = normal_sample.union(anomaly_sample)\n",
    "sample_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11268"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_anomaly_sample = anomaly_sample.sample(False, 0.25)\n",
    "min_normal_sample = normal_sample.sample(False, 0.25)\n",
    "min_sample_df = min_normal_sample.union(min_anomaly_sample)\n",
    "min_sample_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9005, 2263)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = min_sample_df.randomSplit([0.8, 0.2], seed=12345)\n",
    "train_df.count(), test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1391.4850676359997 seconds\n",
      "\n",
      "Time: 23.19141779393333 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logistic_regression = LogisticRegression(featuresCol = 'features', labelCol = 'label_nr')\n",
    "\n",
    "start = timeit.default_timer()\n",
    "logistic_regression_model = logistic_regression.fit(train_df)\n",
    "#logistic_regression_model = logistic_regression.train(train_df)\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "run_time = stop - start\n",
    "\n",
    "print(\"Time: {} seconds\\n\".format(run_time))\n",
    "print(\"Time: {} minutes\\n\".format(run_time/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------+--------------------+\n",
      "|         probability|       rawPrediction|prediction|label_nr|            features|\n",
      "+--------------------+--------------------+----------+--------+--------------------+\n",
      "|[0.99999999276373...|[18.7441609031566...|       0.0|     0.0|(705567,[0,7,32,1...|\n",
      "|[0.99999999815000...|[20.1080853517952...|       0.0|     0.0|(705567,[0,9,30,1...|\n",
      "|[0.99999999728095...|[19.7229862597264...|       0.0|     0.0|(705567,[0,9,33,1...|\n",
      "|[0.99999999157593...|[18.5921736132090...|       0.0|     0.0|(705567,[0,11,33,...|\n",
      "|[0.99999977519002...|[15.3080101412597...|       0.0|     0.0|(705567,[0,12,31,...|\n",
      "|[0.99999999992509...|[23.3148255108057...|       0.0|     0.0|(705567,[0,12,32,...|\n",
      "|[0.99999998857606...|[18.2875553934923...|       0.0|     0.0|(705567,[0,20,31,...|\n",
      "|[0.99999999461817...|[19.0402389856457...|       0.0|     0.0|(705567,[1,6,32,1...|\n",
      "|[0.99999998242313...|[17.8566823076974...|       0.0|     0.0|(705567,[1,7,33,1...|\n",
      "|[0.99999996731095...|[17.2362258375670...|       0.0|     0.0|(705567,[1,11,29,...|\n",
      "|[0.99999999498090...|[19.1100169757857...|       0.0|     0.0|(705567,[1,13,29,...|\n",
      "|[0.99999998725754...|[18.1783267887567...|       0.0|     0.0|(705567,[2,6,33,1...|\n",
      "|[0.99999935028130...|[14.2467256959296...|       0.0|     0.0|(705567,[2,7,32,1...|\n",
      "|[0.99999997761923...|[17.6150641435115...|       0.0|     0.0|(705567,[2,8,29,1...|\n",
      "|[0.99999998388997...|[17.943823984178,...|       0.0|     0.0|(705567,[2,8,29,1...|\n",
      "|[0.99999999009677...|[18.4304055665213...|       0.0|     0.0|(705567,[2,8,32,1...|\n",
      "|[0.99999999977444...|[22.2124739109691...|       0.0|     0.0|(705567,[2,8,34,1...|\n",
      "|[0.99999999988679...|[22.9018534863676...|       0.0|     0.0|(705567,[2,10,34,...|\n",
      "|[0.99999998207304...|[17.8369606378681...|       0.0|     0.0|(705567,[2,11,32,...|\n",
      "|[0.99999998318445...|[17.9009619460700...|       0.0|     0.0|(705567,[2,14,29,...|\n",
      "+--------------------+--------------------+----------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_logistic_regression_predictions_df = logistic_regression_model.transform(test_df)\n",
    "\n",
    "test_logistic_regression_predictions_df.select(\"probability\",\"rawPrediction\", \"prediction\", \"label_nr\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FPR</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.966697</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.978398</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.990999</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          FPR       TPR\n",
       "0    0.000000  0.000000\n",
       "1    0.000000  0.050347\n",
       "2    0.000000  0.114583\n",
       "3    0.000000  0.214410\n",
       "4    0.000000  0.276910\n",
       "..        ...       ...\n",
       "113  0.966697  1.000000\n",
       "114  0.978398  1.000000\n",
       "115  0.990999  1.000000\n",
       "116  1.000000  1.000000\n",
       "117  1.000000  1.000000\n",
       "\n",
       "[118 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_summary = logistic_regression_model.evaluate(test_df)\n",
    "\n",
    "roc = test_summary.roc.toPandas()\n",
    "roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC Curve on Test Data = 1\n"
     ]
    }
   ],
   "source": [
    "evaluator_roc_area = BinaryClassificationEvaluator(rawPredictionCol = \"rawPrediction\", \n",
    "                                                   labelCol = 'label_nr',\n",
    "                                                   metricName = \"areaUnderROC\")\n",
    "\n",
    "print(\"Area Under ROC Curve on Test Data = %g\" % evaluator_roc_area.evaluate(test_logistic_regression_predictions_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------+--------------------+\n",
      "|         probability|       rawPrediction|prediction|label_nr|            features|\n",
      "+--------------------+--------------------+----------+--------+--------------------+\n",
      "|[0.99999999276373...|[18.7441609031566...|       0.0|     0.0|(705567,[0,7,32,1...|\n",
      "|[0.99999999815000...|[20.1080853517952...|       0.0|     0.0|(705567,[0,9,30,1...|\n",
      "|[0.99999999728095...|[19.7229862597264...|       0.0|     0.0|(705567,[0,9,33,1...|\n",
      "|[0.99999999157593...|[18.5921736132090...|       0.0|     0.0|(705567,[0,11,33,...|\n",
      "|[0.99999977519002...|[15.3080101412597...|       0.0|     0.0|(705567,[0,12,31,...|\n",
      "|[0.99999999992509...|[23.3148255108057...|       0.0|     0.0|(705567,[0,12,32,...|\n",
      "|[0.99999998857606...|[18.2875553934923...|       0.0|     0.0|(705567,[0,20,31,...|\n",
      "|[0.99999999461817...|[19.0402389856457...|       0.0|     0.0|(705567,[1,6,32,1...|\n",
      "|[0.99999998242313...|[17.8566823076974...|       0.0|     0.0|(705567,[1,7,33,1...|\n",
      "|[0.99999996731095...|[17.2362258375670...|       0.0|     0.0|(705567,[1,11,29,...|\n",
      "|[0.99999999498090...|[19.1100169757857...|       0.0|     0.0|(705567,[1,13,29,...|\n",
      "|[0.99999998725754...|[18.1783267887567...|       0.0|     0.0|(705567,[2,6,33,1...|\n",
      "|[0.99999935028130...|[14.2467256959296...|       0.0|     0.0|(705567,[2,7,32,1...|\n",
      "|[0.99999997761923...|[17.6150641435115...|       0.0|     0.0|(705567,[2,8,29,1...|\n",
      "|[0.99999998388997...|[17.943823984178,...|       0.0|     0.0|(705567,[2,8,29,1...|\n",
      "|[0.99999999009677...|[18.4304055665213...|       0.0|     0.0|(705567,[2,8,32,1...|\n",
      "|[0.99999999977444...|[22.2124739109691...|       0.0|     0.0|(705567,[2,8,34,1...|\n",
      "|[0.99999999988679...|[22.9018534863676...|       0.0|     0.0|(705567,[2,10,34,...|\n",
      "|[0.99999998207304...|[17.8369606378681...|       0.0|     0.0|(705567,[2,11,32,...|\n",
      "|[0.99999998318445...|[17.9009619460700...|       0.0|     0.0|(705567,[2,14,29,...|\n",
      "+--------------------+--------------------+----------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_logistic_regression_predictions_df.select(\"probability\",\"rawPrediction\", \"prediction\", \"label_nr\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[1111.,    0.],\n",
      "             [   0., 1152.]])\n"
     ]
    }
   ],
   "source": [
    "predictions_and_label_rdd = test_logistic_regression_predictions_df.select(\"prediction\", \"label_nr\").rdd\n",
    "\n",
    "metrics = MulticlassMetrics(predictions_and_label_rdd)\n",
    "\n",
    "print(metrics.confusionMatrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1111.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1152.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TP      FP\n",
       "0  1111.0     0.0\n",
       "1     0.0  1152.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = metrics.confusionMatrix().toArray().tolist()\n",
    "cm_df = spark.createDataFrame(rows, ['TP','FP'])\n",
    "cm_pdf = cm_df.toPandas()\n",
    "cm_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 0.3765047064196512\n"
     ]
    }
   ],
   "source": [
    "# Print the intercept for logistic regression\n",
    "print(\"Intercept: \" + str(logistic_regression_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coefficients for logistic regression\n",
    "#print(\"Coefficients: \" + str(logistic_regression_model.coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_summary = logistic_regression_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objectiveHistory:\n",
      "0.693146681114956\n",
      "0.12987922812990033\n",
      "0.0807684729363865\n",
      "0.06860689523050088\n",
      "0.029863472250343574\n",
      "0.01376323839005431\n",
      "0.006115840715742139\n",
      "0.00309332518632462\n",
      "0.0015341108150373937\n",
      "0.0008165979899110044\n",
      "0.00030901997485115906\n",
      "0.00019316961581520428\n",
      "0.00010355024685040142\n",
      "3.339964099308997e-05\n",
      "2.255809259750256e-05\n",
      "1.0421109021290287e-05\n",
      "5.6523930444413356e-06\n",
      "2.917628493767206e-06\n",
      "2.3913362758669004e-06\n",
      "1.1312344590865928e-06\n",
      "6.731059113930506e-07\n",
      "2.7122776151139885e-07\n",
      "1.5411388921239668e-07\n",
      "7.831104962653362e-08\n",
      "4.025636293694728e-08\n",
      "1.997064595309959e-08\n",
      "1.7064572105254703e-08\n",
      "8.880672756181992e-09\n",
      "4.605132817119638e-09\n",
      "2.3279540440762627e-09\n",
      "1.1846634474685746e-09\n"
     ]
    }
   ],
   "source": [
    "# Obtain the objective per iteration\n",
    "objectiveHistory = training_summary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "|FPR|                TPR|\n",
      "+---+-------------------+\n",
      "|0.0|                0.0|\n",
      "|0.0|0.12868870645662303|\n",
      "|0.0|0.27579321056134903|\n",
      "|0.0| 0.4018193920568005|\n",
      "|0.0| 0.4859108054138008|\n",
      "|0.0| 0.6035056578655424|\n",
      "|0.0| 0.6538717550477036|\n",
      "|0.0| 0.7299755935211892|\n",
      "|0.0| 0.7399600621255824|\n",
      "|0.0| 0.7495007765697803|\n",
      "|0.0| 0.7619258930552474|\n",
      "|0.0| 0.7810073219436432|\n",
      "|0.0| 0.7949855779897936|\n",
      "|0.0| 0.8105169735966274|\n",
      "|0.0|  0.816729531839361|\n",
      "|0.0|  0.827379631684047|\n",
      "|0.0| 0.8338140670068782|\n",
      "|0.0| 0.8446860439316619|\n",
      "|0.0| 0.8504548480142001|\n",
      "|0.0| 0.8619924561792767|\n",
      "+---+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "areaUnderROC: 0.9999966456999896\n"
     ]
    }
   ],
   "source": [
    "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "training_summary.roc.show()\n",
    "print(\"areaUnderROC: \" + str(training_summary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|         threshold|          F-Measure|\n",
      "+------------------+-------------------+\n",
      "|0.9999999999999873|0.22803223904069195|\n",
      "|0.9999999999999778|0.43234782608695654|\n",
      "|0.9999999999999589| 0.5732826843937955|\n",
      "|0.9999999999999509| 0.6540241899357921|\n",
      "|0.9999999999999343| 0.7527328075273281|\n",
      "|0.9999999999999245| 0.7907163938824792|\n",
      "|0.9999999999998912| 0.8439143260228293|\n",
      "|0.9999999999998446| 0.8505483295077786|\n",
      "|0.9999999999997502| 0.8568167406467977|\n",
      "|0.9999999999996954|   0.86487847878101|\n",
      "|0.9999999999996054| 0.8770399900336365|\n",
      "|0.9999999999995162| 0.8857849196538937|\n",
      "| 0.999999999999371| 0.8953431372549019|\n",
      "|0.9999999999992826| 0.8991206643869077|\n",
      "|0.9999999999991058| 0.9055366682855756|\n",
      "|0.9999999999989218| 0.9093768905021175|\n",
      "|0.9999999999984943| 0.9158046668270388|\n",
      "|0.9999999999980054| 0.9191846522781774|\n",
      "|0.9999999999954687| 0.9258817921830315|\n",
      "|0.9999999999909115| 0.9307236061684461|\n",
      "+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the model threshold to maximize F-Measure\n",
    "fMeasure = training_summary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']).select('threshold').head()['threshold']\n",
    "\n",
    "fMeasure.show()\n",
    "#logistic_regression.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Row(max(F-Measure)=0.9995560488346282), 0.9999999299699358)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxFMeasure, bestThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (16) Stop the Spark Context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
