{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/administrator/Development/python/environments/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/administrator/Development/python/environments/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/administrator/Development/python/environments/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/administrator/Development/python/environments/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/administrator/Development/python/environments/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/administrator/Development/python/environments/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../ml_utils')\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import timeit\n",
    "import data_utils as du\n",
    "import spark_utils as su\n",
    "import autoencoder as aenc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark\n",
    "import keras\n",
    "import tensorflow\n",
    "\n",
    "from pyspark.sql import SQLContext, SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "if platform.system() == 'Darwin':\n",
    "    spark_master = \"spark://spark.home.net:7077\"\n",
    "    base_dir = '/Users/administrator/'\n",
    "else:\n",
    "    spark_master = \"spark://lasvegas:7077\"\n",
    "    base_dir = '/home/administrator/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark  = SparkSession.builder \\\n",
    "                  .master(spark_master) \\\n",
    "                  .appName(\"Deep Learning\") \\\n",
    "                  .enableHiveSupport() \\\n",
    "                  .config('spark.jars.packages', 'databricks:spark-deep-learning:1.5.0-spark2.4-s_2.11') \\\n",
    "                  .getOrCreate()\n",
    "\n",
    "#spark.conf.set(\"spark.executor.memory\", '8g')\n",
    "#spark.conf.set('spark.executor.cores', '3')\n",
    "#spark.conf.set('spark.cores.max', '3')\n",
    "#spark.conf.set(\"spark.driver.memory\",'8g')\n",
    "#spark.conf.set(“spark.sql.shuffle.partitions”, 6)\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark:  2.4.4\n",
      "pandas:  0.25.3\n",
      "numpy:  1.17.4\n",
      "Keras:  2.2.4\n",
      "Tensorflow:  1.13.1\n"
     ]
    }
   ],
   "source": [
    "from sparkdl import KerasTransformer\n",
    "\n",
    "print(\"pyspark: \", pyspark.__version__)\n",
    "print(\"pandas: \", pd.__version__)\n",
    "print(\"numpy: \", np.__version__)\n",
    "print(\"Keras: \", keras.__version__)\n",
    "print(\"Tensorflow: \", tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the log statements based on a given schema\n",
    "\n",
    "log_entries_df = sqlContext.read.format('com.databricks.spark.csv') \\\n",
    "    .schema(su.feature_schema_all_string) \\\n",
    "    .options(header = 'false', inferschema = 'false', delimiter = '\\t') \\\n",
    "    .load('./../../shared/data/swissid_authorize_logs_april_to_sept_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter 'bad' statements and select a subset of all features\n",
    "reduced_df = su.clean_log_entries(log_entries_df, True, False, False, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduced_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = reduced_df.schema.names\n",
    "categorical_columns.remove('label_nr')\n",
    "#categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = su.build_scaled_features_pipeline(categorical_columns)\n",
    "pipeline_model = pipeline.fit(reduced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_stop = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model_path = base_dir + \"Development/workspaces/datascience/masterarbeit/shared/models/autoencoder/spark_autoencoder_pipeline_model.hdf5\"\n",
    "\n",
    "pipeline_model.write().overwrite().save(pipeline_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline_model = Pipeline.load(pipeline_model_path)\n",
    "\n",
    "#df = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+\n",
      "|            features|         feature_vec|label_nr|\n",
      "+--------------------+--------------------+--------+\n",
      "|(10,[1,7,8,9],[0....|(10,[1,7,8,9],[1....|       2|\n",
      "|(10,[1,7,8,9],[1....|(10,[1,7,8,9],[2....|       0|\n",
      "|(10,[1,7,9],[1.68...|(10,[1,7,9],[2.0,...|       2|\n",
      "|(10,[1,7,9],[1.68...|(10,[1,7,9],[2.0,...|       2|\n",
      "|(10,[1,2,7,8,9],[...|(10,[1,2,7,8,9],[...|       2|\n",
      "+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df = pipeline_model.transform(reduced_df).select(['features', 'feature_vec', 'label_nr'])\n",
    "features_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4769169"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_df = features_df.filter(\"label_nr = 0.0\")\n",
    "normal_count = normal_df.count()\n",
    "normal_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "762170"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_sample = normal_df.sample(False, 0.16)\n",
    "normal_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(609545, 152625)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the index-labelled Scaled Feature Vectors into Training and Test DataFrames\n",
    "\n",
    "train_df, test_df = normal_sample.randomSplit([0.8, 0.2], seed=12345)\n",
    "train_df.count(), test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = su.convert_feature_vector_to_list(train_df)\n",
    "x_train_pdf = pd.DataFrame(x_train, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = su.convert_feature_vector_to_list(test_df)\n",
    "x_test_pdf = pd.DataFrame(x_test, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/administrator/Development/python/environments/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 44        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                50        \n",
      "=================================================================\n",
      "Total params: 94\n",
      "Trainable params: 94\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_dim = 10\n",
    "encoding_dim = 4\n",
    "\n",
    "model = aenc.create_sparse_auto_encoder(input_dim, encoding_dim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder_start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/administrator/Development/python/environments/masterthesis/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 609545 samples, validate on 152625 samples\n",
      "Epoch 1/2\n",
      "609545/609545 [==============================] - 24s 39us/step - loss: 0.9666 - mean_absolute_error: 0.3435 - acc: 0.8955 - val_loss: 0.9396 - val_mean_absolute_error: 0.3388 - val_acc: 0.9245\n",
      "Epoch 2/2\n",
      "609545/609545 [==============================] - 23s 38us/step - loss: 0.9525 - mean_absolute_error: 0.3399 - acc: 0.9172 - val_loss: 0.9388 - val_mean_absolute_error: 0.3387 - val_acc: 0.9245\n"
     ]
    }
   ],
   "source": [
    "history = aenc.auto_encoder_fit(model, x_train_pdf, x_test_pdf, 'RMSprop', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder_stop = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = base_dir + \"Development/workspaces/datascience/masterarbeit/shared/models/autoencoder/spark_sparse_autoencoder_model.hdf5\"\n",
    "model.save(model_path)\n",
    "\n",
    "history_path = base_dir + \"Development/workspaces/datascience/masterarbeit/shared/models/autoencoder/spark_sparse_autoencoder_history\"\n",
    "du.save_model_history(history, history_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_stop = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Time: 4.367078569816667 minutes\n",
      "\n",
      "Auto-Encoder Time: 0.7841705017000005 minutes\n",
      "\n",
      "Pipeline Time: 1.5728168885166665 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall Time: {} minutes\\n\".format((overall_stop - overall_start)/60))\n",
    "print(\"Auto-Encoder Time: {} minutes\\n\".format((auto_encoder_stop - auto_encoder_start)/60))\n",
    "print(\"Pipeline Time: {} minutes\\n\".format((pipeline_stop - pipeline_start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+\n",
      "|            features|         feature_vec|label_nr|\n",
      "+--------------------+--------------------+--------+\n",
      "|(10,[0,1,2],[6.07...|(10,[0,1,2],[1.0,...|       0|\n",
      "|(10,[0,1,2],[6.07...|(10,[0,1,2],[1.0,...|       0|\n",
      "|(10,[0,1,2],[6.07...|(10,[0,1,2],[1.0,...|       0|\n",
      "|(10,[0,1,2,7],[6....|(10,[0,1,2,7],[1....|       0|\n",
      "|(10,[0,1,2,7],[6....|(10,[0,1,2,7],[1....|       0|\n",
      "+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values_df = su.convert_feature_vector_to_X_df(test_df, sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /private/var/folders/8g/p7382l613rv7bs8x8qx2hbf00000gn/T/spark-117b6a13-c498-446f-8469-833d492e6236/userFiles-3d10021e-94f5-414f-af63-174f516cebb8/databricks_spark-deep-learning-1.5.0-spark2.4-s_2.11.jar/sparkdl/graph/utils.py:220: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /Users/administrator/Development/python/environments/masterthesis/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 4 variables.\n",
      "INFO:tensorflow:Converted 4 variables to const ops.\n",
      "INFO:tensorflow:Froze 0 variables.\n",
      "INFO:tensorflow:Converted 0 variables to const ops.\n",
      "WARNING:tensorflow:From /Users/administrator/Development/python/environments/masterthesis/lib/python3.7/site-packages/tensorflow/python/tools/optimize_for_inference_lib.py:113: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.remove_training_nodes\n"
     ]
    }
   ],
   "source": [
    "# here we use the spark KerasTransformer\n",
    "predictions_df = su.predict(model_path, test_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, features, labels = su.convert_prediction_df_to_lists(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pdf = pd.DataFrame(features, columns=categorical_columns)\n",
    "y_pred = pd.DataFrame(predictions, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    " \n",
    "# load model\n",
    "auto_encoder = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152625/152625 [==============================] - 2s 14us/step\n",
      "Test loss: 0.9387733298454687\n",
      "Test mean_absolute_error: 0.3386950209623082\n",
      "Test acc: 0.9244946764947009\n"
     ]
    }
   ],
   "source": [
    "score = auto_encoder.evaluate(x_test_pdf, x_test_pdf, verbose=1)\n",
    "\n",
    "for i, metric in enumerate(auto_encoder.metrics_names):\n",
    "    print('Test ' + metric + ':', score[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    152625\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99    152625\n",
      "   macro avg       0.50      0.49      0.50    152625\n",
      "weighted avg       1.00      0.99      0.99    152625\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/administrator/Development/python/environments/masterthesis/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "threshold = 10.0\n",
    "\n",
    "y_class, errors = aenc.classify(x_test_pdf, y_pred, threshold)\n",
    "\n",
    "print(classification_report(labels, y_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (16) Stop the Spark Context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
